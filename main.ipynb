{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from natsort import natsorted\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_files(file):\n",
    "    if 'txt' in file:\n",
    "        with open(f'Articles/'+file, 'r', encoding='latin1') as f:\n",
    "            return f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "for file in os.listdir('Articles'):\n",
    "    documents.append(read_files(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Phase $:-$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_docs = []\n",
    "for document in documents:\n",
    "    token_docs.append(word_tokenize(document))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove in , to from stop words\n",
    "#### Add some extra punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words.remove('in')\n",
    "stop_words.remove('to')\n",
    "\n",
    "stop_words.extend([\".\", \",\", \"'\", \"-\", \"_\", \":\", \"(\", \")\", \"&\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "for token in token_docs:\n",
    "    each_token = []\n",
    "    for term in token:\n",
    "        if term not in stop_words:\n",
    "            each_token.append(term)\n",
    "    documents.append(each_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['antony', 'brutus', 'caeser', 'cleopatra', 'mercy', 'worser']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second phase $:-$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement function to do all steps in first phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(doc):\n",
    "    token_docs = word_tokenize(doc)\n",
    "    stop_words = stopwords.words('english')\n",
    "    stop_words.remove('in')\n",
    "    stop_words.remove('to')\n",
    "    stop_words.extend([\".\", \",\", \"'\", \"-\", \"_\", \":\", \"(\", \")\", \"&\", \"/\", \"\\\\\", \"]\", \"[\", \"''\", '\"\"', \"' '\", '\" \"'])\n",
    "    prepared_doc = []\n",
    "    for term in token_docs:\n",
    "        if term not in stop_words:\n",
    "            prepared_doc.append(term)\n",
    "    return prepared_doc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the stemmer.\n",
    "stemmer = PorterStemmer()\n",
    " \n",
    "# Initialize the file no.\n",
    "fileno = 0\n",
    " \n",
    "# Initialize the dictionary.\n",
    "pos_index = {}\n",
    " \n",
    "# Initialize the file mapping (fileno -> file name).\n",
    "file_map = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1.txt', '2.txt', '3.txt', '4.txt', '5.txt', '6.txt', '7.txt', '8.txt', '9.txt', '10.txt']\n"
     ]
    }
   ],
   "source": [
    "# Open files.\n",
    "file_names = natsorted(os.listdir(\"Articles\"))\n",
    "print(file_names)\n",
    "# For every file.\n",
    "for file_name in file_names:\n",
    "\n",
    "    # Read file contents.\n",
    "    with open(f'Articles/{file_name}', 'r', encoding='latin1') as f:\n",
    "        stuff = f.read()\n",
    "    # This is the list of words in order of the text.\n",
    "    # We need to preserve the order because we require positions.\n",
    "    # 'preprocessing' function does some basic punctuation removal,\n",
    "    # stopword removal etc.\n",
    "    final_token_list = preprocessing(stuff)\n",
    "\n",
    "    # For position and term in the tokens.\n",
    "    for pos, term in enumerate(final_token_list):\n",
    "        # print(pos, '-->' ,term)\n",
    "        # First stem the term.\n",
    "        term = stemmer.stem(term)\n",
    "        # print(term)\n",
    "        # If term already exists in the positional index dictionary.\n",
    "        if term in pos_index:\n",
    "                \n",
    "            # Increment total freq by 1.\n",
    "            pos_index[term][0] = pos_index[term][0] + 1\n",
    "                \n",
    "            # Check if the term has existed in that DocID before.\n",
    "            if fileno in pos_index[term][1]:\n",
    "                pos_index[term][1][fileno].append(pos)\n",
    "                    \n",
    "            else:\n",
    "                pos_index[term][1][fileno] = [pos]\n",
    "\n",
    "        # If term does not exist in the positional index dictionary\n",
    "        # (first encounter).\n",
    "        else:\n",
    "                \n",
    "            # Initialize the list.\n",
    "            pos_index[term] = []\n",
    "            # The total frequency is 1.\n",
    "            pos_index[term].append(1)\n",
    "            # The postings list is initially empty.\n",
    "            pos_index[term].append({})     \n",
    "            # Add doc ID to postings list.\n",
    "            pos_index[term][1][fileno] = [pos]\n",
    "\n",
    "    # Map the file no. to the file name.\n",
    "    file_map[fileno] = \"test/\" + file_name\n",
    "\n",
    "    # Increment the file no. counter for document ID mapping             \n",
    "    fileno += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### displays each term "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'antoni': [3, {0: [0], 1: [0], 5: [0]}],\n",
       " 'brutu': [3, {0: [1], 1: [1], 3: [0]}],\n",
       " 'caeser': [5, {0: [2], 1: [2], 3: [1], 4: [0], 5: [1]}],\n",
       " 'cleopatra': [1, {0: [3]}],\n",
       " 'merci': [5, {0: [4], 2: [0], 3: [2], 4: [1], 5: [2]}],\n",
       " 'worser': [4, {0: [5], 2: [1], 3: [3], 4: [2]}],\n",
       " 'calpurnia': [1, {1: [3]}],\n",
       " 'angel': [3, {6: [0], 7: [0], 8: [0]}],\n",
       " 'fool': [4, {6: [1], 7: [1], 8: [1], 9: [0]}],\n",
       " 'fear': [3, {6: [2], 7: [2], 9: [1]}],\n",
       " 'in': [4, {6: [3], 7: [3], 8: [2], 9: [2]}],\n",
       " 'rush': [4, {6: [4], 7: [4], 8: [3], 9: [3]}],\n",
       " 'to': [4, {6: [5], 7: [5], 8: [4], 9: [4]}],\n",
       " 'tread': [4, {6: [6], 7: [6], 8: [5], 9: [5]}]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Allow users to write phrase query "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "term fool and the count is 4\n",
      "6 : [1]\n",
      "7 : [1]\n",
      "8 : [1]\n",
      "9 : [0]\n"
     ]
    }
   ],
   "source": [
    "test_term = input()\n",
    "test_pos_index = pos_index[test_term]\n",
    "print('term', test_term, 'and the count is', test_pos_index[0])\n",
    "for doc in test_pos_index[1]:\n",
    "    print(doc, ':', test_pos_index[1][doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Third phase $:-$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "for file in os.listdir('Articles'):\n",
    "    documents.append(\" \".join(preprocessing(read_files(file))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a TfidfVectorizer object\n",
    "vectorizer = TfidfVectorizer()\n",
    "# It fits the data and transform it as a vector\n",
    "X = vectorizer.fit_transform(documents)\n",
    "# Convert the X as transposed matrix\n",
    "X = X.T.toarray()\n",
    "# Create a DataFrame and set the vocabulary as the index\n",
    "df = pd.DataFrame(X, index=vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>angels</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.409883</td>\n",
       "      <td>0.409883</td>\n",
       "      <td>0.449365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>antony</th>\n",
       "      <td>0.412627</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.474292</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.662993</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brutus</th>\n",
       "      <td>0.412627</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.474292</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.571154</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>caeser</th>\n",
       "      <td>0.329457</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.378692</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.456030</td>\n",
       "      <td>0.555563</td>\n",
       "      <td>0.529358</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>calpurnia</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.637721</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cleopatra</th>\n",
       "      <td>0.554808</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fear</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.449365</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.409883</td>\n",
       "      <td>0.409883</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fools</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.399518</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.364415</td>\n",
       "      <td>0.364415</td>\n",
       "      <td>0.399518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>in</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.399518</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.364415</td>\n",
       "      <td>0.364415</td>\n",
       "      <td>0.399518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mercy</th>\n",
       "      <td>0.329457</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.668165</td>\n",
       "      <td>0.456030</td>\n",
       "      <td>0.555563</td>\n",
       "      <td>0.529358</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rush</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.399518</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.364415</td>\n",
       "      <td>0.364415</td>\n",
       "      <td>0.399518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.399518</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.364415</td>\n",
       "      <td>0.364415</td>\n",
       "      <td>0.399518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tread</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.399518</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.364415</td>\n",
       "      <td>0.364415</td>\n",
       "      <td>0.399518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worser</th>\n",
       "      <td>0.366855</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.744013</td>\n",
       "      <td>0.507796</td>\n",
       "      <td>0.618628</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  0         1         2         3         4         5  \\\n",
       "angels     0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "antony     0.412627  0.000000  0.474292  0.000000  0.000000  0.000000   \n",
       "brutus     0.412627  0.000000  0.474292  0.000000  0.571154  0.000000   \n",
       "caeser     0.329457  0.000000  0.378692  0.000000  0.456030  0.555563   \n",
       "calpurnia  0.000000  0.000000  0.637721  0.000000  0.000000  0.000000   \n",
       "cleopatra  0.554808  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "fear       0.000000  0.449365  0.000000  0.000000  0.000000  0.000000   \n",
       "fools      0.000000  0.399518  0.000000  0.000000  0.000000  0.000000   \n",
       "in         0.000000  0.399518  0.000000  0.000000  0.000000  0.000000   \n",
       "mercy      0.329457  0.000000  0.000000  0.668165  0.456030  0.555563   \n",
       "rush       0.000000  0.399518  0.000000  0.000000  0.000000  0.000000   \n",
       "to         0.000000  0.399518  0.000000  0.000000  0.000000  0.000000   \n",
       "tread      0.000000  0.399518  0.000000  0.000000  0.000000  0.000000   \n",
       "worser     0.366855  0.000000  0.000000  0.744013  0.507796  0.618628   \n",
       "\n",
       "                  6         7         8         9  \n",
       "angels     0.000000  0.409883  0.409883  0.449365  \n",
       "antony     0.662993  0.000000  0.000000  0.000000  \n",
       "brutus     0.000000  0.000000  0.000000  0.000000  \n",
       "caeser     0.529358  0.000000  0.000000  0.000000  \n",
       "calpurnia  0.000000  0.000000  0.000000  0.000000  \n",
       "cleopatra  0.000000  0.000000  0.000000  0.000000  \n",
       "fear       0.000000  0.409883  0.409883  0.000000  \n",
       "fools      0.000000  0.364415  0.364415  0.399518  \n",
       "in         0.000000  0.364415  0.364415  0.399518  \n",
       "mercy      0.529358  0.000000  0.000000  0.000000  \n",
       "rush       0.000000  0.364415  0.364415  0.399518  \n",
       "to         0.000000  0.364415  0.364415  0.399518  \n",
       "tread      0.000000  0.364415  0.364415  0.399518  \n",
       "worser     0.000000  0.000000  0.000000  0.000000  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similar_articles(q, df):\n",
    "    print(\"query:\", q)\n",
    "    # Convert the query become a vector\n",
    "    q = [q]\n",
    "    q_vec = vectorizer.transform(q).toarray().reshape(df.shape[0],)\n",
    "    sim = {}  # Calculate the similarity\n",
    "    for i in range(10):\n",
    "        sim[i] = np.dot(df.loc[:, i].values, q_vec) / np.linalg.norm(df.loc[:, i]) * np.linalg.norm(q_vec)\n",
    "    \n",
    "    # Sort the values \n",
    "    sim_sorted = sorted(sim.items(), key=lambda x: x[1], reverse=True)\n",
    "    # # Print the articles and their similarity values\n",
    "    # for k, v in sim_sorted:\n",
    "    #     if v != 0.0:\n",
    "    #         print(\"Similarity value:\", v)\n",
    "    #         print(\"The article is:\", documents[sim_sorted[k]])\n",
    "\n",
    "    print(\"Similarity value:\", sim_sorted[0][1])\n",
    "    print(\"The article is:\", documents[sim_sorted[0][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: fools fear\n",
      "Similarity value: 0.6012844514534148\n",
      "The article is: fools fear in rush to tread\n"
     ]
    }
   ],
   "source": [
    "q1 = 'fools fear'# Call the function\n",
    "get_similar_articles(q1, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('python_10')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b35187bd8e81c46a491b0701dacbc6455fb6bb49fda2ea4057c13cf77d84decb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
