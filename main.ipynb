{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/create-a-simple-search-engine-using-python-412587619ff5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "# import re\n",
    "# import string\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# import pandas as pd\n",
    "# import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrap documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a request to the website\n",
    "r = requests.get('https://bola.kompas.com/')\n",
    "# Create an object to parse the HTML format\n",
    "soup = BeautifulSoup(r.content, 'html.parser')\n",
    "# Retrieve all popular news links (Fig. 1)\n",
    "link = []\n",
    "for i in soup.find('div', {'class':'most__wrap'}).find_all('a'):\n",
    "    i['href'] = i['href'] + '?page=all'\n",
    "    link.append(i['href'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each link, we retrieve paragraphs from it, combine each paragraph as one string, and save it to documents (Fig. 2)\n",
    "documents = []\n",
    "for i in link:\n",
    "    # Make a request to the link\n",
    "    r = requests.get(i)\n",
    "  \n",
    "    # Initialize BeautifulSoup object to parse the content \n",
    "    soup = BeautifulSoup(r.content, 'html.parser')\n",
    "  \n",
    "    # Retrieve all paragraphs and combine it as one\n",
    "    sen = []\n",
    "    for i in soup.find('div', {'class':'read__content'}).find_all('p'):\n",
    "        sen.append(i.text)\n",
    "\n",
    "    # Add the combined paragraphs to documents\n",
    "    documents.append(' '.join(sen))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(documents)):\n",
    "    with open(f'document_{i+1}.txt', 'a', encoding=\"utf-8\") as f:\n",
    "        f.write(documents[i])\n",
    "        f.close"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "for file in os.listdir():\n",
    "    if 'txt' in file:\n",
    "        with open(file, 'r', encoding='latin1') as f:\n",
    "            documents.append(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_docs = []\n",
    "for document in documents:\n",
    "    token_docs.append(word_tokenize(document))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words.remove('in')\n",
    "stop_words.remove('to')\n",
    "\n",
    "stop_words.extend([\".\", \",\", \"'\", \"-\", \"_\", \":\", \"(\", \")\", \"&\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "for token in token_docs:\n",
    "    each_token = []\n",
    "    for term in token:\n",
    "        if term not in stop_words:\n",
    "            each_token.append(term)\n",
    "    documents.append(each_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_clean = []\n",
    "for d in documents:\n",
    "    # Remove Unicode\n",
    "    document_test = re.sub(r'[^\\x00-\\x7F]+', ' ', d)\n",
    "    # Remove Mentions\n",
    "    document_test = re.sub(r'@\\w+', '', document_test)\n",
    "    # Lowercase the document\n",
    "    document_test = document_test.lower()\n",
    "    # Remove punctuations\n",
    "    document_test = re.sub(r'[%s]' % re.escape(string.punctuation), ' ', document_test)\n",
    "    # Lowercase the numbers\n",
    "    document_test = re.sub(r'[0-9]', '', document_test)\n",
    "    # Remove the doubled space\n",
    "    document_test = re.sub(r'\\s{2,}', ' ', document_test)\n",
    "    documents_clean.append(document_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>aaron</th>\n",
       "      <td>0.026937</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.256663</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.032908</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abu</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.102258</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ac</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.234746</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ada</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.034354</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.032599</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.101841</td>\n",
       "      <td>0.033808</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>adalah</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.027891</td>\n",
       "      <td>0.027162</td>\n",
       "      <td>0.026466</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020670</td>\n",
       "      <td>0.027448</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.033925</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               0         1         2         3         4         5         6  \\\n",
       "aaron   0.026937  0.000000  0.000000  0.256663  0.000000  0.000000  0.000000   \n",
       "abu     0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.102258   \n",
       "ac      0.000000  0.000000  0.000000  0.000000  0.234746  0.000000  0.000000   \n",
       "ada     0.000000  0.034354  0.000000  0.032599  0.000000  0.101841  0.033808   \n",
       "adalah  0.000000  0.027891  0.027162  0.026466  0.000000  0.020670  0.027448   \n",
       "\n",
       "               7         8    9  \n",
       "aaron   0.032908  0.000000  0.0  \n",
       "abu     0.000000  0.000000  0.0  \n",
       "ac      0.000000  0.000000  0.0  \n",
       "ada     0.000000  0.000000  0.0  \n",
       "adalah  0.000000  0.033925  0.0  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate a TfidfVectorizer object\n",
    "vectorizer = TfidfVectorizer()\n",
    "# It fits the data and transform it as a vector\n",
    "X = vectorizer.fit_transform(documents_clean)\n",
    "# Convert the X as transposed matrix\n",
    "X = X.T.toarray()\n",
    "# Create a DataFrame and set the vocabulary as the index\n",
    "df = pd.DataFrame(X, index=vectorizer.get_feature_names_out())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similar_articles(q, df):\n",
    "  print(\"query:\", q)\n",
    "  print(\"Berikut artikel dengan nilai cosine similarity tertinggi: \")\n",
    "  # Convert the query become a vector\n",
    "  q = [q]\n",
    "  q_vec = vectorizer.transform(q).toarray().reshape(df.shape[0],)\n",
    "  # print('q_vec', q_vec)\n",
    "  sim = {}\n",
    "  # Calculate the similarity\n",
    "  for i in range(10):\n",
    "    sim[i] = np.dot(df.loc[:, i].values, q_vec) / np.linalg.norm(df.loc[:, i]) * np.linalg.norm(q_vec)\n",
    "  \n",
    "  # Sort the values \n",
    "  sim_sorted = sorted(sim.items(), key=lambda x: x[1], reverse=True)\n",
    "  # Print the articles and their similarity values\n",
    "  for k, v in sim_sorted:\n",
    "    # print(k, '----', v)\n",
    "    if v != 0.0:\n",
    "      print(\"Nilai Similaritas:\", v)\n",
    "      print(documents_clean[k])\n",
    "      print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: Real\n",
      "Berikut artikel dengan nilai cosine similarity tertinggi: \n",
      "q_vec [0. 0. 0. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Add The Query\n",
    "q1 = 'Real'\n",
    "# Call the function\n",
    "get_similar_articles(q1, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('python_10')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b35187bd8e81c46a491b0701dacbc6455fb6bb49fda2ea4057c13cf77d84decb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
