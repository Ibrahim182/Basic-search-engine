{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/create-a-simple-search-engine-using-python-412587619ff5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from natsort import natsorted\n",
    "from nltk.stem import PorterStemmer\n",
    "# import re\n",
    "# import string\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# import pandas as pd\n",
    "# import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrap documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a request to the website\n",
    "r = requests.get('https://bola.kompas.com/')\n",
    "# Create an object to parse the HTML format\n",
    "soup = BeautifulSoup(r.content, 'html.parser')\n",
    "# Retrieve all popular news links (Fig. 1)\n",
    "link = []\n",
    "for i in soup.find('div', {'class':'most__wrap'}).find_all('a'):\n",
    "    i['href'] = i['href'] + '?page=all'\n",
    "    link.append(i['href'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each link, we retrieve paragraphs from it, combine each paragraph as one string, and save it to documents (Fig. 2)\n",
    "documents = []\n",
    "for i in link:\n",
    "    # Make a request to the link\n",
    "    r = requests.get(i)\n",
    "  \n",
    "    # Initialize BeautifulSoup object to parse the content \n",
    "    soup = BeautifulSoup(r.content, 'html.parser')\n",
    "  \n",
    "    # Retrieve all paragraphs and combine it as one\n",
    "    sen = []\n",
    "    for i in soup.find('div', {'class':'read__content'}).find_all('p'):\n",
    "        sen.append(i.text)\n",
    "\n",
    "    # Add the combined paragraphs to documents\n",
    "    documents.append(' '.join(sen))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(documents)):\n",
    "    with open(f'document_{i+1}.txt', 'a', encoding=\"utf-8\") as f:\n",
    "        f.write(documents[i])\n",
    "        f.close"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "for file in os.listdir():\n",
    "    if 'txt' in file:\n",
    "        with open(file, 'r', encoding='latin1') as f:\n",
    "            documents.append(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Phase $:-$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_docs = []\n",
    "for document in documents:\n",
    "    token_docs.append(word_tokenize(document))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove in , to from stop words\n",
    "#### Add some extra punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words.remove('in')\n",
    "stop_words.remove('to')\n",
    "\n",
    "stop_words.extend([\".\", \",\", \"'\", \"-\", \"_\", \":\", \"(\", \")\", \"&\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "for token in token_docs:\n",
    "    each_token = []\n",
    "    for term in token:\n",
    "        if term not in stop_words:\n",
    "            each_token.append(term)\n",
    "    documents.append(each_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second phase $:-$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement function to do all steps in first phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(doc):\n",
    "    token_docs = word_tokenize(doc)\n",
    "    stop_words = stopwords.words('english')\n",
    "    stop_words.remove('in')\n",
    "    stop_words.remove('to')\n",
    "    stop_words.extend([\".\", \",\", \"'\", \"-\", \"_\", \":\", \"(\", \")\", \"&\"])\n",
    "    prepared_doc = []\n",
    "    for term in token:\n",
    "        if term not in stop_words:\n",
    "            prepared_doc.append(term)\n",
    "    return prepared_doc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the stemmer.\n",
    "stemmer = PorterStemmer()\n",
    " \n",
    "# Initialize the file no.\n",
    "fileno = 0\n",
    " \n",
    "# Initialize the dictionary.\n",
    "pos_index = {}\n",
    " \n",
    "# Initialize the file mapping (fileno -> file name).\n",
    "file_map = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open files.\n",
    "file_names = natsorted(os.listdir(\"Articles\"))\n",
    "\n",
    "# For every file.\n",
    "for file_name in file_names:\n",
    "\n",
    "    # Read file contents.\n",
    "    with open(f'Articles/{file_name}', 'r', encoding='latin1') as f:\n",
    "        stuff = f.read()\n",
    "    # This is the list of words in order of the text.\n",
    "    # We need to preserve the order because we require positions.\n",
    "    # 'preprocessing' function does some basic punctuation removal,\n",
    "    # stopword removal etc.\n",
    "    final_token_list = preprocessing(stuff)\n",
    "\n",
    "    # For position and term in the tokens.\n",
    "    for pos, term in enumerate(final_token_list):\n",
    "            \n",
    "        # First stem the term.\n",
    "        term = stemmer.stem(term)\n",
    "\n",
    "        # If term already exists in the positional index dictionary.\n",
    "        if term in pos_index:\n",
    "                \n",
    "            # Increment total freq by 1.\n",
    "            pos_index[term][0] = pos_index[term][0] + 1\n",
    "                \n",
    "            # Check if the term has existed in that DocID before.\n",
    "            if fileno in pos_index[term][1]:\n",
    "                pos_index[term][1][fileno].append(pos)\n",
    "                    \n",
    "            else:\n",
    "                pos_index[term][1][fileno] = [pos]\n",
    "\n",
    "        # If term does not exist in the positional index dictionary\n",
    "        # (first encounter).\n",
    "        else:\n",
    "                \n",
    "            # Initialize the list.\n",
    "            pos_index[term] = []\n",
    "            # The total frequency is 1.\n",
    "            pos_index[term].append(1)\n",
    "            # The postings list is initially empty.\n",
    "            pos_index[term].append({})     \n",
    "            # Add doc ID to postings list.\n",
    "            pos_index[term][1][fileno] = [pos]\n",
    "\n",
    "    # Map the file no. to the file name.\n",
    "    file_map[fileno] = \"Articles/\" + file_name\n",
    "\n",
    "    # Increment the file no. counter for document ID mapping             \n",
    "    fileno += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positional Index\n",
      "[380, {0: [1, 23, 37, 49, 65, 112, 150, 207, 250, 280, 298, 316, 435, 496, 566, 576, 606, 622, 646], 1: [1, 23, 37, 49, 65, 112, 150, 207, 250, 280, 298, 316, 435, 496, 566, 576, 606, 622, 646], 2: [1, 23, 37, 49, 65, 112, 150, 207, 250, 280, 298, 316, 435, 496, 566, 576, 606, 622, 646], 3: [1, 23, 37, 49, 65, 112, 150, 207, 250, 280, 298, 316, 435, 496, 566, 576, 606, 622, 646], 4: [1, 23, 37, 49, 65, 112, 150, 207, 250, 280, 298, 316, 435, 496, 566, 576, 606, 622, 646], 5: [1, 23, 37, 49, 65, 112, 150, 207, 250, 280, 298, 316, 435, 496, 566, 576, 606, 622, 646], 6: [1, 23, 37, 49, 65, 112, 150, 207, 250, 280, 298, 316, 435, 496, 566, 576, 606, 622, 646], 7: [1, 23, 37, 49, 65, 112, 150, 207, 250, 280, 298, 316, 435, 496, 566, 576, 606, 622, 646], 8: [1, 23, 37, 49, 65, 112, 150, 207, 250, 280, 298, 316, 435, 496, 566, 576, 606, 622, 646], 9: [1, 23, 37, 49, 65, 112, 150, 207, 250, 280, 298, 316, 435, 496, 566, 576, 606, 622, 646], 10: [1, 23, 37, 49, 65, 112, 150, 207, 250, 280, 298, 316, 435, 496, 566, 576, 606, 622, 646], 11: [1, 23, 37, 49, 65, 112, 150, 207, 250, 280, 298, 316, 435, 496, 566, 576, 606, 622, 646], 12: [1, 23, 37, 49, 65, 112, 150, 207, 250, 280, 298, 316, 435, 496, 566, 576, 606, 622, 646], 13: [1, 23, 37, 49, 65, 112, 150, 207, 250, 280, 298, 316, 435, 496, 566, 576, 606, 622, 646], 14: [1, 23, 37, 49, 65, 112, 150, 207, 250, 280, 298, 316, 435, 496, 566, 576, 606, 622, 646], 15: [1, 23, 37, 49, 65, 112, 150, 207, 250, 280, 298, 316, 435, 496, 566, 576, 606, 622, 646], 16: [1, 23, 37, 49, 65, 112, 150, 207, 250, 280, 298, 316, 435, 496, 566, 576, 606, 622, 646], 17: [1, 23, 37, 49, 65, 112, 150, 207, 250, 280, 298, 316, 435, 496, 566, 576, 606, 622, 646], 18: [1, 23, 37, 49, 65, 112, 150, 207, 250, 280, 298, 316, 435, 496, 566, 576, 606, 622, 646], 19: [1, 23, 37, 49, 65, 112, 150, 207, 250, 280, 298, 316, 435, 496, 566, 576, 606, 622, 646]}]\n"
     ]
    }
   ],
   "source": [
    "# Sample positional index to test the code.\n",
    "sample_pos_idx = pos_index[\"timna\"]\n",
    "print(\"Positional Index\")\n",
    "print(sample_pos_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tFilename,\t\t\t\t\t\t [Positions]\n",
      "Articles/document_1.txt [1, 23, 37, 49, 65, 112, 150, 207, 250, 280, 298, 316, 435, 496, 566, 576, 606, 622, 646]\n",
      "Articles/document_2.txt [1, 23, 37, 49, 65, 112, 150, 207, 250, 280, 298, 316, 435, 496, 566, 576, 606, 622, 646]\n",
      "Articles/document_3.txt [1, 23, 37, 49, 65, 112, 150, 207, 250, 280, 298, 316, 435, 496, 566, 576, 606, 622, 646]\n",
      "Articles/document_4.txt [1, 23, 37, 49, 65, 112, 150, 207, 250, 280, 298, 316, 435, 496, 566, 576, 606, 622, 646]\n",
      "Articles/document_5.txt [1, 23, 37, 49, 65, 112, 150, 207, 250, 280, 298, 316, 435, 496, 566, 576, 606, 622, 646]\n",
      "Articles/document_6.txt [1, 23, 37, 49, 65, 112, 150, 207, 250, 280, 298, 316, 435, 496, 566, 576, 606, 622, 646]\n",
      "Articles/document_7.txt [1, 23, 37, 49, 65, 112, 150, 207, 250, 280, 298, 316, 435, 496, 566, 576, 606, 622, 646]\n",
      "Articles/document_8.txt [1, 23, 37, 49, 65, 112, 150, 207, 250, 280, 298, 316, 435, 496, 566, 576, 606, 622, 646]\n",
      "Articles/document_9.txt [1, 23, 37, 49, 65, 112, 150, 207, 250, 280, 298, 316, 435, 496, 566, 576, 606, 622, 646]\n",
      "Articles/document_10.txt [1, 23, 37, 49, 65, 112, 150, 207, 250, 280, 298, 316, 435, 496, 566, 576, 606, 622, 646]\n",
      "Articles/document_1.txt [1, 23, 37, 49, 65, 112, 150, 207, 250, 280, 298, 316, 435, 496, 566, 576, 606, 622, 646]\n",
      "Articles/document_2.txt [1, 23, 37, 49, 65, 112, 150, 207, 250, 280, 298, 316, 435, 496, 566, 576, 606, 622, 646]\n",
      "Articles/document_3.txt [1, 23, 37, 49, 65, 112, 150, 207, 250, 280, 298, 316, 435, 496, 566, 576, 606, 622, 646]\n",
      "Articles/document_4.txt [1, 23, 37, 49, 65, 112, 150, 207, 250, 280, 298, 316, 435, 496, 566, 576, 606, 622, 646]\n",
      "Articles/document_5.txt [1, 23, 37, 49, 65, 112, 150, 207, 250, 280, 298, 316, 435, 496, 566, 576, 606, 622, 646]\n",
      "Articles/document_6.txt [1, 23, 37, 49, 65, 112, 150, 207, 250, 280, 298, 316, 435, 496, 566, 576, 606, 622, 646]\n",
      "Articles/document_7.txt [1, 23, 37, 49, 65, 112, 150, 207, 250, 280, 298, 316, 435, 496, 566, 576, 606, 622, 646]\n",
      "Articles/document_8.txt [1, 23, 37, 49, 65, 112, 150, 207, 250, 280, 298, 316, 435, 496, 566, 576, 606, 622, 646]\n",
      "Articles/document_9.txt [1, 23, 37, 49, 65, 112, 150, 207, 250, 280, 298, 316, 435, 496, 566, 576, 606, 622, 646]\n",
      "Articles/document_10.txt [1, 23, 37, 49, 65, 112, 150, 207, 250, 280, 298, 316, 435, 496, 566, 576, 606, 622, 646]\n"
     ]
    }
   ],
   "source": [
    "file_list = sample_pos_idx[1]\n",
    "print(\"\\tFilename,\\t\\t\\t\\t\\t\\t [Positions]\")\n",
    "for fileno, positions in file_list.items():\n",
    "    print(file_map[fileno], positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('python_10')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b35187bd8e81c46a491b0701dacbc6455fb6bb49fda2ea4057c13cf77d84decb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
